\subsection{Feedback}
\todo{Performance Data wurde als Leistungsdaten übersetzt, und Preference data als
  Präferenzinformationen - ok so?}

Feedback-Methoden können abhängig vom Typ der Studie variieren. Formative Studien profitieren von
qualitativen Methoden, während formative Studien quantitatives Feedback einsetzen. Laut
\textcite{barnumUsabilityTesting2021} kann in beiden Fällen eine Benutzung der jeweils anderen
Herangehensweise die Erkenntnisse erweitern. Wie bereits in \ref{sec:formative-summative}
festgestellt, ist es in zeitigeren Entwicklungsstadien vorteilhafter, formative Studien
durchzuführen. \todo{Überprüfen ob a) stimmig und b) so beschrieben} Qualitative Aussagen von
Teilnehmer:innen können wertvolle Einblicke in die größten Problembereiche geben und auf die
nächsten Entwicklungsschritte hindeuten.
\parencite{barnumUsabilityTesting2021}

\textcite{barnumUsabilityTesting2021} nennt Leistungsdaten und Präferenzinformationen als
quantitative Maße für Usability. Als Leistungsdaten gelten zum Beispiel die Zeit zum Absolvieren von
Aufgaben, Fehlerquote oder Erfolgsrate. Wie bereits in \ref{sec:scenarios} angeschnitten,
spricht eine hohe Erfolgsrate jedoch nur von einem Mindestmaß an Usability und sollte durch weitere
Werte unterstützt werden. Des Weiteren weißt \textcite{barnumUsabilityTesting2021} darauf hin, dass
auch unterschiedliche Grade von Erfolg beim Absolvieren von Aufgaben bestehen. So könnte eine
Nutzer:in den schnellsten, vom Design vorgesehenen Weg zum Ziel benutzen, oder aber auch einen
indirekten Pfad nehmen. Auch verschiedene in Anspruch genommene Hilfstellungen können von Usability
Problemen sprechen. Das Bewerten einer Aufgabe als Misserfolg kann auch verschiedene Gründe haben:
Aufgeben, Abruch durch die Testleitung, oder die Annahme, dass die Aufgabe beendet sei, ohne dass
sie das wirklich ist. Eine weitere Metrik die einfach zu erheben ist, ist die Zeit zum Vollenden der
Aufgaben. In der ersten Studie können diese Werte als Ausgangsbasis gesammelt werden, und in
zukünftigen Studien zum Vergleich benutzt werden, so \textcite{barnumUsabilityTesting2021}. Zu
beachten ist, dass diese einfach zu erhebenden Metriken nicht die gesamte Usability Erfahrung
beschreiben können.
\parencite{barnumUsabilityTesting2021}

Als Präferenzinformationen beschreibt \textcite{barnumUsabilityTesting2021} Antworten auf
Fragebögen, welche nach Aufgaben und nach dem kompletten Test erhoben werden. Befinden sie sich auf
Skalen (1 bis 5 oder 1 bis 10), können sie als quantitative Daten benutzt werden. Fragen mit offenem
Ende lieferen qualitative Informationen über die Erlebnisse der Teilnehmer:innen. Zusätzlich sollten
auch über Thinking-Aloud gewonnene Eindrücke berücksichtigt werden und, wenn verfügbar,
nonverbales Feedback wie Körperspräche oder nonverbale Ausdrücke gesammelt werden.
\parencite{barnumUsabilityTesting2021}

\subsubsection{Fragebögen}
Nach jedem Szenario Feedback über die Aufgabe zu sammeln, hat laut
\textcite{barnumUsabilityTesting2021} den Vorteil, dass die Erinnerungen noch frisch sind. Dabei
kann es sich auch um eine einzige Frage handeln. \textcite{sauroIfYou2010} nennt als Eigenschaften
eines guten Fragebogens, dass er zuverlässig, sensibel, valide und kurz ist, sowie einfach zu
beantworten, zu handhaben und zu bewerten sein sollte. \textcite{barnumUsabilityTesting2021} schlägt
vor eins oder mehr der folgenden Themen abzufragen: Schwierigkeit der Durchführung, benötigte Zeit
(von "weniger als erwartet" bis "mehr als erwartet"), die Wahrscheinlichkeit, dass dieses Feature
erneut benutzt wird, und das Vertrauen in die erfolgreiche Bewältigung der Aufgabe.

\textcite{sauroIfYou2010} listet folgende Standard-Fragebögen für das Einholen von Feedback nach
Aufgaben auf: \ac{asq}, \ac{nasa-tlx}, \ac{smeq}, \ac{ume} und \ac{seq}. \todo{bessere Überleitung?
  maybe selbsterschließend?}

Der \textbf{\ac{asq}} wurde von \textcite{lewisPsychometricEvaluation1991} eingeführt und beinhaltet
drei Fragen bezüglich Schwierigkeit der Aufgabe, Bearbeitungszeit und der Menge der unterstützenden
Informationen (Dokumentation, Online-Hilfe, etc.). Die Antworten werden auf einer Skala von 1
(stimme voll zu) bis 7 (stimme überhaupt nicht zu) angegeben.

Der \textbf{\ac{nasa-tlx}} wurde von \textcite{hartDevelopmentNASATLX1988} entwickelt und berechnet
eine Gesamtbewertung der Arbeitsbelastung basierend auf, geistiger, physischer und zeitlicher
Anforderung, sowie Leistung, Aufwand und Frustration.
\parencite{nasaNASATLX}

Der \textbf{\ac{smeq}}, zuerst von \textcite{zijlstraConstructionScale1985} beschrieben, besteht aus
einer einzelnen Skala von "gar nicht anstrengend" bis "extrem anstrengend".
\todo{schwer vs schwierig?}

Bei \textbf{\ac{ume}} handelt es sich um eine Methode, bei der nach jeder Aufgabe ein numerischer
Wert von den Teilnehmer:innen erfragt wird, welcher sich auf die Aufgabe bezieht. Die erste Antwort
kann arbiträr sein, die restlichen orientieren sich dann aber an den vorherigen
Antworten - somit können die Aufgaben untereinander verglichen werden. Zu
betonen ist, dass die Skala im Vorhinein nicht festgelegt ist, und von den
Teilnehmer:innne selbst gewählt wird.
\parencite{mcgeeUsabilityMagnitude2003}

Die \textbf{\ac{seq}} besteht aus einer einzigen Frage: "Insgesamt war diese Aufgabe...",
wobei von einer Skala von 1 (sehr schwierig) bis 5 (sehr einfach) ausgewählt werden kann.
\textcite{tedescoComparisonMethods2006} stellte fest, dass diese Methode bei einer kleinen
Stichprobengröße die beständigsten Ergebnisse liefert \footnote{Es fand ein Vergleich mit 4 anderen
  Methoden statt: \ac{asq}, \ac{ume}, eine Variation von \ac{seq} und die Erwartungs-Bewertung von
  \textcite{albertThisWhat2003}}. \textcite{sauroComparisonThree2009} verglich die \ac{seq} mit
\ac{smeq} und \ac{ume}. Dabei punktete \ac{seq} mit einfacher Bedienbarkeit und Erlernbarkeit,
während alle Methoden eine ähnliche Zuverlässigkeit aufwiesen.

\textcite{barnumUsabilityTesting2021} erklärt, dass Fragebögen nach Szenarios an die soeben
absolvierte Aufgabe angepasst werden können. Während das gleiche für Fragebögen nach dem gesamten
Test gilt, existieren auch da Standard-Fragebögen, wie \ac{sus}, \ac{csuq} und \ac{nps}.

\textbf{\ac{sus}} wurde von \textcite{brookeSUSQuick1996} \todo{nochmal nachschauen ob das wirklich
  erste Publikation ist, weil \textcite{barnumUsabilityTesting2021} (S.233) von 1986 schreibt (auch in
  Quellen)} vorgestellt und besteht aus 10 Fragen, die auf einer Likert-Skala bewertet werden.
\todo{Muss Likert-Skala erklärt werden?} Um den Gesamtwert-Wert zu berechnen, werden die einzelnen
Antworten miteinander addiert und mit 2,5 multipliziert, um einen \ac{sus}-Wert zwischen 0 und 100
zu erhalten. \textcite{sauroMeasuringUsability2011} führte eine Meta-Analyse von 500 Studien durch
und errechnete einen durchschnittlichen \ac{sus}-Wert von 68. Dieser Werte könnte laut
\textcite{barnumUsabilityTesting2021} als Basiswert für iterative Studien benutzt werden.
\todo{überdurschnittlich und so erklären?}

\textcite{brookeSUSQuick1996} beschreibt folgende Vorgehensweise zur Anwendung von \ac{sus}: Die
Fragen sollten direkt nach dem Test gestellt werden, bevor etwaige andere Aktivitäten durchgeführt
werden. Des Weiteren sollte Teilnehmer:innen ihre direkte Antwort geben, statt zu viel darüber
nachzudenken. Schlussendlich betont \textcite{brookeSUSQuick1996}, dass optimalerweise eine Antwort
auf jede Frage gegeben wird - im Falle von Meinungslosigkeit kann das der Mittelpunkt sein. Laut
\textcite{barnumUsabilityTesting2021} ist die \ac{sus} sehr weit verbreitet, da sie schnell
anzuwenden ist, unabhäng von der Technologie des getesteten Produktes ist und sich über die Zeit als
valide Methode für Studien ab fünf Teilnehmer:innen erwiesen hat.

\textbf{\ac{csuq}} ist sehr ähnlich zum \ac{pssuq}. Beide beinhalten 16 positive Fragen, welche auf
einer Skala mit 7 Punkten bewertet werden. Im Kontrast zu \ac{sus} wird auch eine Bewertung von
"nicht anwendbar" erlaubt. Die Fragebögen ergeben einen Gesamtwert und drei Unterbwertungen von
Systemqualität, Informationsqualität und Oberflächenqualität. \parencite{barnumUsabilityTesting2021}
\todo{vielleicht auch noch Originalquelle für \ac{csuq}/\ac{pssuq} finden?}

Beim \textbf{\ac{nps}} handelt es sich um eine einzige Frage, die ungefähr wie folgt lautet: "Wie
wahrscheinlich ist es, dass Sie [Unternehmen/Produkt] weiterempfehlen?". Dabei wird die Antwort auf
einer Skala zwischen 0 und 10 gegeben. Teilnehmer:innen, die einen Wert unter 7 angeben, werden als
\textit{Detractors} bezeichnet, bei einer Antwort über 8 werden sie als \textit{Promoters}
eingestuft. Der \ac{nps}-Wert wird aus einer Subtraktion des Prozentsatzes der \textit{Detractors}
vom Prozentsatz der \textit{Promoters} gewonnen und kann somit zwischen -100 und 100 liegen.
\todo{explizites Plus?} Aufgrund einer Untersuchung der Korrelation zwischen \ac{sus} und
\ac{nps} von \textcite{sauroDoesBetter2010} stellt \textcite{barnumUsabilityTesting2021} fest, dass
eine Daumenregel zum errechnen des \ac{nps}-Wertes ist, den \ac{sus}-Wert durch 10 zu teilen.
\todo{Vielleicht noch Kritik am NPS finden?}
