\subsection{Feedback}
\todo{Performance Data wurde als Leistungsdaten übersetzt, und Preference data als
Präferenzinformationen - ok so?}

Feedback-Methoden können abhängig vom Typ der Studie variieren. Formative Studien profitieren von
qualitativen Methoden, während formative Studien quantitatives Feedback einsetzen. Laut
\textcite{barnumUsabilityTesting2021} kann in beiden Fällen eine Benutzung der jeweils anderen
Herangehensweise die Erkenntnisse erweitern. Wie bereits in \ref{section:formative-summative}
festgestellt, ist es in zeitigeren Entwicklungsstadien vorteilhafter, formative Studien
durchzuführen. \todo{Überprüfen ob a) stimmig und b) so beschrieben} Qualitative Aussagen von
Teilnehmer:innen können wertvolle Einblicke in die größten Problembereiche geben und auf die
nächsten Entwicklungsschritte hindeuten.
\parencite{barnumUsabilityTesting2021}

\textcite{barnumUsabilityTesting2021} nennt Leistungsdaten und Präferenzinformationen als
quantitative Maße für Usability. Als Leistungsdaten gelten zum Beispiel die Zeit zum Absolvieren von
Aufgaben, Fehlerquote oder Erfolgsrate. Wie bereits in \ref{section:scenarios} angeschnitten,
spricht eine hohe Erfolgsrate jedoch nur von einem Mindestmaß an Usability und sollte durch weitere
Werte unterstützt werden. Des Weiteren weißt \textcite{barnumUsabilityTesting2021} darauf hin, dass
auch unterschiedliche Grade von Erfolg beim Absolvieren von Aufgaben bestehen. So könnte eine
Nutzer:in den schnellsten, vom Design vorgesehenen Weg zum Ziel benutzen, oder aber auch einen
indirekten Pfad nehmen. Auch verschiedene in Anspruch genommene Hilfstellungen können von Usability
Problemen sprechen. Das Bewerten einer Aufgabe als Misserfolg kann auch verschiedene Gründe haben:
Aufgeben, Abruch durch die Testleitung, oder die Annahme, dass die Aufgabe beendet sei, ohne dass
sie das wirklich ist. Eine weitere Metrik die einfach zu erheben ist, ist die Zeit zum Vollenden der
Aufgaben. In der ersten Studie können diese Werte als Ausgangsbasis gesammelt werden, und in
zukünftigen Studien zum Vergleich benutzt werden, so \textcite{barnumUsabilityTesting2021}. Zu
beachten ist, dass diese einfach zu erhebenden Metriken nicht die gesamte Usability Erfahrung
beschreiben können.
\parencite{barnumUsabilityTesting2021}

Als Präferenzinformationen beschreibt \textcite{barnumUsabilityTesting2021} Antworten auf
Fragebögen, welche nach Aufgaben und nach dem kompletten Test erhoben werden. Befinden sie sich auf
Skalen (1 bis 5 oder 1 bis 10), können sie als quantitative Daten benutzt werden. Fragen mit offenem
Ende lieferen qualitative Informationen über die Erlebnisse der Teilnehmer:innen. Zusätzlich sollten
auch über Thinking-Aloud gewonnene Eindrücke berücksichtigt werden und, wenn verfügbar,
nonverbales Feedback wie Körperspräche oder nonverbale Ausdrücke gesammelt werden.
\parencite{barnumUsabilityTesting2021}

\subsubsection{Fragebögen}
Nach jedem Szenario Feedback über die Aufgabe zu sammeln, hat laut
\textcite{barnumUsabilityTesting2021} den Vorteil, dass die Erinnerungen noch frisch sind. Dabei
kann es sich auch um eine einzige Frage handeln. \textcite{sauroIfYou2010} nennt als Eigenschaften
eines guten Fragebogens, dass er zuverlässig, sensibel, valide und kurz ist, sowie einfach zu
beantworten, zu handhaben und zu bewerten sein sollte. \textcite{barnumUsabilityTesting2021} schlägt
vor eins oder mehr der folgenden Themen abzufragen: Schwierigkeit der Durchführung, benötigte Zeit
(von "weniger als erwartet" bis "mehr als erwartet"), die Wahrscheinlichkeit, dass dieses Feature
erneut benutzt wird, und das Vertrauen in die erfolgreiche Bewältigung der Aufgabe.

\textcite{sauroIfYou2010} listet folgende Standard-Fragebögen für das Einholen von Feedback nach
Aufgaben auf: \ac{ASQ}, \ac{NASA-TLX}, \ac{SMEQ}, \ac{UME} und \ac{SEQ}. \todo{bessere Überleitung?
maybe selbsterschließend?}

Der \textbf{\ac{ASQ}} wurde von \textcite{lewisPsychometricEvaluation1991} eingeführt und beinhaltet
drei Fragen bezüglich Schwierigkeit der Aufgabe, Bearbeitungszeit und der Menge der unterstützenden
Informationen (Dokumentation, Online-Hilfe, etc.). Die Antworten werden auf einer Skala von 1
(stimme voll zu) bis 7 (stimme überhaupt nicht zu) angegeben.

Der \textbf{\ac{NASA-TLX}} wurde von \textcite{hartDevelopmentNASATLX1988} entwickelt und berechnet
eine Gesamtbewertung der Arbeitsbelastung basierend auf, geistiger, physischer und zeitlicher
Anforderung, sowie Leistung, Aufwand und Frustration.
\parencite{nasaNASATLX}

Der \textbf{\ac{SMEQ}}, zuerst von \textcite{zijlstraConstructionScale1985} beschrieben, besteht aus
einer einzelnen Skala von "gar nicht anstrengend" bis "extrem anstrengend".
\todo{schwer vs schwierig?}

Bei \textbf{\ac{UME}} handelt es sich um eine Methode, bei der nach jeder Aufgabe ein numerischer
Wert von den Teilnehmer:innen erfragt wird, welcher sich auf die Aufgabe bezieht. Die erste Antwort
kann arbiträr sein, die restlichen orientieren sich dann aber an den vorherigen
Antworten - somit können die Aufgaben untereinander verglichen werden. Zu
betonen ist, dass die Skala im Vorhinein nicht festgelegt ist, und von den
Teilnehmer:innne selbst gewählt wird.
\parencite{mcgeeUsabilityMagnitude2003}

Die \textbf{\ac{SEQ}} besteht aus einer einzigen Frage: "Insgesamt war diese Aufgabe...",
wobei von einer Skala von 1 (sehr schwierig) bis 5 (sehr einfach) ausgewählt werden kann.
\textcite{tedescoComparisonMethods2006} stellte fest, dass diese Methode bei einer kleinen
Stichprobengröße die beständigsten Ergebnisse liefert \footnote{Es fand ein Vergleich mit 4 anderen
Methoden statt: \ac{ASQ}, \ac{UME}, eine Variation von \ac{SEQ} und die Erwartungs-Bewertung von
\textcite{albertThisWhat2003}}. \textcite{sauroComparisonThree2009} verglich die \ac{SEQ} mit
\ac{SMEQ} und \ac{UME}. Dabei punktete \ac{SEQ} mit einfacher Bedienbarkeit und Erlernbarkeit,
während alle Methoden eine ähnliche Zuverlässigkeit aufwiesen.
